from collections import defaultdict,Counter
from typing import List,Union,Dict,DefaultDict,Tuple
import regex
import heapq
import re
import os
from tqdm import tqdm
from pathlib import Path
import time
from functools import partial
import random
import mmap
import multiprocessing
# text -> é¢„åˆ†è¯ -> é¢„åˆ†è¯utf-8ç¼–ç  -> åˆå§‹åŒ–å…¨å±€ç´¢å¼•è¡¨ -> è¿­ä»£æ›´æ–° -> è·å¾—å…¨å±€ç´¢å¼•è¡¨ä½œä¸ºè¯è¡¨
GPT2_SPLIT_PATTERN = r"'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"
global_worker_byte_map = None
def load_and_sample_data(file_path: str, sample_size: int = 22000, special_token: str = "<|endoftext|>") -> str:
    """å†…å­˜æ˜ å°„æ–¹å¼åŠ è½½å¹¶é‡‡æ ·æ–‡æ¡£"""
    try:
        with open(file_path, "r+", encoding='utf-8', errors='ignore') as f:
            with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:
                documents = []
                start = 0
                while start < len(mm):
                    end = mm.find(special_token.encode('utf-8'), start)
                    if end == -1:
                        doc = mm[start:].decode('utf-8', errors='replace').strip()
                        if doc:
                            documents.append(doc)
                        break
                    
                    doc = mm[start:end].decode('utf-8', errors='replace').strip()
                    if doc:
                        documents.append(doc)
                    start = end + len(special_token)
                
                if len(documents) > sample_size:
                    documents = random.sample(documents, sample_size)
                
                return special_token.join(documents)
    except Exception as e:
        raise IOError(f"åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")
    
    
def init_worker(byte_map: Dict[int, str]):
    global global_worker_byte_map
    global_worker_byte_map = byte_map

def pre_tokenize_worker(doc: str) -> List[List[str]]:
    return pre_tokenize_document(doc, global_worker_byte_map)
def gpt2_byte_to_unicode_local():
    bs = list(range(33,127))+list(range(161,173))+list(range(174,256))
    cs = bs[:]
    n = 0
    for i in range(256):
        if i not in bs:
            cs.append(256+n)
            bs.append(i)
            n+=1
    return {b:chr(c) for c,b in zip(cs,bs)}

def pre_tokenize_document(doc: str, bytes_to_unicode_map: Dict[int, str]) -> List[List[str]]:
    """é¢„åˆ†è¯å¤„ç†å•ä¸ªæ–‡æ¡£"""
    tokens = regex.findall(GPT2_SPLIT_PATTERN, doc, flags=regex.UNICODE)
    sequences = []
    for token in tokens:
        token_unicode = ''.join(bytes_to_unicode_map[b] for b in token.encode('utf-8'))
        sequences.append(list(token_unicode))
    return sequences

def parallel_pre_tokenize(documents: List[str], num_processes: int, bytes_to_unicode_map: Dict[int, str]) -> List[List[str]]:
    """å¹¶è¡Œé¢„åˆ†è¯ä¼˜åŒ–"""
    if num_processes <= 1:
        return [seq for doc in documents for seq in pre_tokenize_document(doc, bytes_to_unicode_map)]
    
    with multiprocessing.Pool(
        num_processes,
        initializer=init_worker,
        initargs=(bytes_to_unicode_map,)
    ) as pool:
        results = list(tqdm(
            pool.imap(pre_tokenize_worker, documents, chunksize=50),
            total=len(documents),
            desc="é¢„åˆ†è¯",
            mininterval=1
        ))
    return [seq for doc_sequences in results for seq in doc_sequences]

        
class BPE:
    def __init__(self,sequences):
        self.sequences = sequences
        self.pair_counts:DefaultDict[Tuple[str,str],int] = defaultdict(int)
        self.pair_positions: DefaultDict[Tuple[str,str],List[Tuple[int,int]]]=defaultdict(list)
        self.heap = []
        self.heap_entries:DefaultDict[Tuple[str,str],int] = defaultdict(int)
        self.init_pair()
        self.init_heap()
    
    def init_heap(self):
        for pair,count in self.pair_counts:
            if count>1:
                entry = (-count,pair)
                heapq.heappush(self.heap,entry)
                self.heap_entries[entry]+=1
                
    def init_pair(self):
        for i,text in enumerate(self.sequences):
            for s in range(len(text)-1):
                l,r = text[s],text[s+1]
                self.pair_positions[(l,r)].append((i,s))
                self.pair_counts[(l,r)]+=1
                
    def get_most_frequent(self):
        while self.heap:
            pair,count = self.heap[0]
            if pair not in self.heap_entries:
                heapq.heappop(self.heap)
                continue
            
            current_count = self.heap_entries[pair]
            if -count == current_count and current_count > 10:
                return pair
            heapq.heappop(self.heap)
            if pair in self.heap_entries:
                del self.heap_entries[pair]
        return None
    
    def _add_position(self,pair:Tuple[str,str],seq_inx:int,pos:int):
        self.pair_positions[pair].append((seq_inx,pos))
        
    def _update_pair_count(self,pair:Tuple[str,str],delta:int):
        if delta == 0:
            return
        if pair not in self.pair_counts:
            self.pair_counts[pair] = 0
        new_count = self.pair_counts[pair] + delta
        self.pair_counts[pair] = new_count
        if new_count < 0:
            new_count = 0
            self.pair_counts[pair] = 0
        if pair in self.heap_entries and self.heap_entries[pair] is not None:
            self.heap_entries[pair][0] = -new_count
        elif new_count > 1:
            entry = [-new_count, pair]
            heapq.heappush(self.heap, entry)
            self.heap_entries[pair] = entry
            
            
    def merge_pair(self, pair: Tuple[str, str], new_token: str) -> int:
        """åˆå¹¶å­—ç¬¦å¯¹å¹¶æ›´æ–°ç´¢å¼•"""
        if pair not in self.pair_positions or not self.pair_positions[pair]:
            return 0
        
        # æŒ‰åºåˆ—å’Œä½ç½®åˆ†ç»„
        positions_by_seq = defaultdict(list)
        for seq_idx, pos in self.pair_positions[pair]:
            positions_by_seq[seq_idx].append(pos)
        
        merge_count = 0
        for seq_idx, positions in positions_by_seq.items():
            seq = self.sequences[seq_idx]
            # æŒ‰ä½ç½®å€’åºæ’åº
            positions.sort(reverse=True)
            last_merged_pos = -2
            
            for pos in positions:
                # æ£€æŸ¥æ˜¯å¦å·²è¢«å‰é¢çš„åˆå¹¶å½±å“
                if pos >= len(seq) - 1 or pos <= last_merged_pos:
                    continue
                if seq[pos] != pair[0] or seq[pos + 1] != pair[1]:
                    continue
                
                # æ‰§è¡Œåˆå¹¶
                seq[pos] = new_token
                del seq[pos + 1]
                merge_count += 1
                last_merged_pos = pos
                
                # æ›´æ–°å·¦ä¾§pair
                if pos > 0:
                    left_pair = (seq[pos - 1], pair[0])
                    self._update_pair_count(left_pair, -1)
                    
                    new_left_pair = (seq[pos - 1], new_token)
                    self._update_pair_count(new_left_pair, 1)
                    self._add_position(new_left_pair, seq_idx, pos - 1)
                
                # æ›´æ–°å³ä¾§pair
                if pos < len(seq) - 1:
                    right_pair = (pair[1], seq[pos + 1])
                    self._update_pair_count(right_pair, -1)
                    
                    new_right_pair = (new_token, seq[pos + 1])
                    self._update_pair_count(new_right_pair, 1)
                    self._add_position(new_right_pair, seq_idx, pos)
        
        # æ¸…ç†å·²åˆå¹¶çš„pair
        if pair in self.pair_counts:
            del self.pair_counts[pair]
        if pair in self.pair_positions:
            del self.pair_positions[pair]
        if pair in self.heap_entries:
            # æ ‡è®°ä¸ºæ— æ•ˆï¼Œç¨åæ¸…ç†
            self.heap_entries[pair] = None
        
        return merge_count
                

def run_train_bpe(
    input_path: Union[str, os.PathLike],
    vocab_size: int,
    special_tokens: List[str] = ["<|endoftext|>"],
    num_processes: int = 8,
    sample_size: int = 22000,
    **kwargs,
) -> Tuple[Dict[int, bytes], List[Tuple[bytes, bytes]]]:
    # å‚æ•°éªŒè¯
    base_vocab_size = 256 + len(special_tokens)
    if vocab_size < base_vocab_size:
        raise ValueError(f"vocab_sizeè‡³å°‘éœ€{base_vocab_size}")
    
    # 1. å­—èŠ‚åˆ°Unicodeæ˜ å°„
    bytes_to_unicode_map = gpt2_byte_to_unicode_local()
    unicode_to_bytes_map = {v: bytes([k]) for k, v in bytes_to_unicode_map.items()}
    
    # 2. åˆå§‹åŒ–è¯æ±‡è¡¨
    vocab = {i: bytes([i]) for i in range(256)}
    next_token_id = 256
    existing_bytes = set(vocab.values())
    
    # 3. æ·»åŠ ç‰¹æ®Štoken
    for st in special_tokens:
        st_bytes = st.encode("utf-8")
        if st_bytes not in existing_bytes and len(vocab) < vocab_size:
            vocab[next_token_id] = st_bytes
            existing_bytes.add(st_bytes)
            next_token_id += 1
    
    # 4. åŠ è½½å¹¶é‡‡æ ·æ•°æ®
    print(f"ğŸ“– ä» {input_path} åŠ è½½å¹¶é‡‡æ · {sample_size} ä¸ªæ–‡æ¡£...")
    text = load_and_sample_data(input_path, sample_size, special_tokens[0])
    
    # 5. åˆ†å‰²æ–‡æ¡£
    escaped_tokens = [re.escape(st) for st in special_tokens]   ## è¿”å› "<\|endoftext\|>"
    split_pattern = "|".join(escaped_tokens) 
    documents = [part for part in re.split(split_pattern, text) if part]
    
    # 6. å¹¶è¡Œé¢„åˆ†è¯
    sequences = parallel_pre_tokenize(documents, num_processes, bytes_to_unicode_map)
    print(f"âœ… é¢„åˆ†è¯å®Œæˆï¼Œå¾—åˆ° {len(sequences):,} ä¸ªtokenåºåˆ—")
    
    # 7. åˆå§‹åŒ–ç´¢å¼•ç»“æ„
    print("ğŸ”§ æ„å»ºBPEç´¢å¼•...")
    bpe_index = BPE(sequences)
    merges = []
    vocab_progress = len(vocab)
    total_merges = vocab_size - vocab_progress
    
    # 8. BPEè®­ç»ƒä¸»å¾ªç¯
    print(f"ğŸ”„ å¼€å§‹BPEè®­ç»ƒï¼Œç›®æ ‡åˆå¹¶æ•°: {total_merges:,}")
    progress_bar = tqdm(total=total_merges, desc="è®­ç»ƒBPE", unit="åˆå¹¶", mininterval=0.5)
    
    while vocab_progress < vocab_size:
        best_pair = bpe_index.get_most_frequent()
        if best_pair is None:
            print("\nâš ï¸ æ²¡æœ‰æ›´å¤šæœ‰æ•ˆçš„å­—ç¬¦å¯¹å¯ä¾›åˆå¹¶ï¼Œæå‰ç»“æŸè®­ç»ƒ")
            break
        
        # åˆ›å»ºæ–°token
        new_token_str = best_pair[0] + best_pair[1]
        p1_bytes = unicode_to_bytes_map[best_pair[0]]
        p2_bytes = unicode_to_bytes_map[best_pair[1]]
        new_token_bytes = p1_bytes + p2_bytes
        
        # æ‰§è¡Œåˆå¹¶
        merge_count = bpe_index.merge_pair(best_pair, new_token_str)
        if merge_count == 0:
            continue
        
        # æ›´æ–°è¯æ±‡è¡¨
        if new_token_bytes not in existing_bytes:
            vocab[next_token_id] = new_token_bytes
            existing_bytes.add(new_token_bytes)
            merges.append((p1_bytes, p2_bytes))
            next_token_id += 1
            vocab_progress += 1
            progress_bar.update(1)
        
        # æ›´æ–°æ˜ å°„è¡¨
        unicode_to_bytes_map[new_token_str] = new_token_bytes
    
    progress_bar.close()
    return vocab, merges

def evaluate_tokenizer(vocab: Dict[int, bytes], merges: List[Tuple[bytes, bytes]], test_text: str):
    """ç®€å•è¯„ä¼°åˆ†è¯å™¨æ•ˆæœ"""
    print("\nğŸ” åˆ†è¯å™¨è¯„ä¼°")
    sample_text = test_text[:200] + "..." if len(test_text) > 200 else test_text
    print(f"æ ·ä¾‹æ–‡æœ¬: {sample_text}")
    
    # ç®€å•ç»Ÿè®¡
    unique_tokens = set(vocab.values())
    print(f"è¯æ±‡è¡¨å¤§å°: {len(vocab):,}")
    print(f"å”¯ä¸€tokenæ•°: {len(unique_tokens):,}")
    print(f"åˆå¹¶æ“ä½œæ•°: {len(merges):,}")

if __name__ == "__main__":
    # é…ç½®å‚æ•°
    config = {
        "vocab_size": 10000,
        "special_tokens": ["<|endoftext|>", "<pad>", "<unk>"],
        "num_processes": 8,
        "sample_size": 22000,  # åˆå§‹é‡‡æ ·22,000æ–‡æ¡£
    }
    
    # æ•°æ®é›†è·¯å¾„
    train_path = "/home/mw/input/cs336_129682968/TinyStoriesV2-GPT4-train.txt"
    valid_path = "/home/mw/input/cs336_129682968/TinyStoriesV2-GPT4-valid.txt"
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(train_path).exists():
        raise FileNotFoundError(f"è®­ç»ƒé›†æ–‡ä»¶ {train_path} ä¸å­˜åœ¨")
    if not Path(valid_path).exists():
        raise FileNotFoundError(f"éªŒè¯é›†æ–‡ä»¶ {valid_path} ä¸å­˜åœ¨")
    
    # è®­ç»ƒæ¨¡å‹
    print("ğŸš€ å¼€å§‹è®­ç»ƒ")
    start_time = time.time()
    
    train_vocab, train_merges = run_train_bpe(train_path, **config)
    
    print(f"\nâœ… è®­ç»ƒå®Œæˆ! è€—æ—¶: {time.time() - start_time:.2f}ç§’")
    
    # å°è§„æ¨¡éªŒè¯ (ä½¿ç”¨éªŒè¯é›†çš„10%)
    print("\nğŸ”¬ å°è§„æ¨¡éªŒè¯")
    valid_config = config.copy()
    valid_config["sample_size"] = int(2)  # éªŒè¯é›†ä½¿ç”¨500æ–‡æ¡£ (10%)
    
    valid_vocab, valid_merges = run_train_bpe(valid_path, **valid_config)
    
    # åˆ†æç»“æœ
    print("\nğŸ“Š è®­ç»ƒç»“æœ")
    print(f"è®­ç»ƒè¯æ±‡è¡¨å¤§å°: {len(train_vocab):,}")
    print(f"è®­ç»ƒåˆå¹¶æ“ä½œæ•°: {len(train_merges):,}")
    print(f"éªŒè¯è¯æ±‡è¡¨å¤§å°: {len(valid_vocab):,}")
    print(f"éªŒè¯åˆå¹¶æ“ä½œæ•°: {len(valid_merges):,}")
    
    # æ¯”è¾ƒè¯æ±‡è¡¨é‡å ç‡
    train_tokens = set(train_vocab.values())
    valid_tokens = set(valid_vocab.values())
    overlap = train_tokens & valid_tokens
    print(f"\nğŸ“ˆ è¯æ±‡è¡¨é‡å ç‡: {len(overlap)/len(train_tokens):.1%}")
    
    # åŠ è½½éªŒè¯é›†æ ·ä¾‹è¿›è¡Œè¯„ä¼°
    with open(valid_path, "r", encoding="utf-8") as f:
        valid_text = f.read(1000)  # è¯»å–å‰1000å­—ç¬¦ç”¨äºè¯„ä¼°
    evaluate_tokenizer(train_vocab, train_merges, valid_text)

    import json  # éœ€è¦å¯¼å…¥jsonæ¨¡å—

    # åœ¨mainå‡½æ•°æœ«å°¾æ·»åŠ ä»¥ä¸‹ä»£ç ï¼ˆåœ¨å†…å­˜åˆ†æä¹‹å‰ï¼‰
    def save_vocab_and_merges(vocab: Dict[int, bytes], merges: List[Tuple[bytes, bytes]], vocab_path: str, merges_path: str):
        """ä¿å­˜è¯æ±‡è¡¨å’Œåˆå¹¶åˆ—è¡¨åˆ°æ–‡ä»¶"""
        # 1. ä¿å­˜è¯æ±‡è¡¨ (JSONæ ¼å¼)
        vocab_str = {idx: token.decode('utf-8', errors='replace') for idx, token in vocab.items()}
        with open(vocab_path, 'w', encoding='utf-8') as f:
            json.dump(vocab_str, f, ensure_ascii=False, indent=2)
        
        # 2. ä¿å­˜åˆå¹¶åˆ—è¡¨ (æ–‡æœ¬æ ¼å¼)
        with open(merges_path, 'w', encoding='utf-8') as f:
            for merge in merges:
                part1 = merge[0].decode('utf-8', errors='replace')
                part2 = merge[1].decode('utf-8', errors='replace')
                f.write(f"{part1} {part2}\n")

    # åœ¨mainå‡½æ•°ä¸­è°ƒç”¨ä¿å­˜åŠŸèƒ½ï¼ˆåœ¨è®­ç»ƒå®Œæˆåï¼‰
    output_dir = "/home/mw/project"  # ä¿®æ”¹ä¸ºæ‚¨çš„è¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)

    vocab_path = os.path.join(output_dir, "gpt2_vocab.json")
    merges_path = os.path.join(output_dir, "gpt2_merges.txt")

    save_vocab_and_merges(train_vocab, train_merges, vocab_path, merges_path)
    print(f"âœ… è¯æ±‡è¡¨å·²ä¿å­˜è‡³: {vocab_path}")
    print(f"âœ… åˆå¹¶åˆ—è¡¨å·²ä¿å­˜è‡³: {merges_path}")

    # å†…å­˜åˆ†æ
    import psutil
    process = psutil.Process()
    mem_usage = process.memory_info().rss / (1024 ** 3)  # GB
    print(f"ğŸ’¾ å³°å€¼å†…å­˜ä½¿ç”¨: {mem_usage:.2f} GB")
    